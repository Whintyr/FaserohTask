{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11176414,"sourceType":"datasetVersion","datasetId":6975613}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport math\nimport copy\nimport nltk\nimport torch\nimport pickle\nimport random\nimport fractions\nimport numpy as np\nimport sympy as sp\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:28.363861Z","iopub.execute_input":"2025-04-01T15:14:28.364207Z","iopub.status.idle":"2025-04-01T15:14:28.369602Z","shell.execute_reply.started":"2025-04-01T15:14:28.364182Z","shell.execute_reply":"2025-04-01T15:14:28.368794Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"seed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:28.375959Z","iopub.execute_input":"2025-04-01T15:14:28.376167Z","iopub.status.idle":"2025-04-01T15:14:28.387104Z","shell.execute_reply.started":"2025-04-01T15:14:28.376148Z","shell.execute_reply":"2025-04-01T15:14:28.386235Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:28.388288Z","iopub.execute_input":"2025-04-01T15:14:28.388589Z","iopub.status.idle":"2025-04-01T15:14:28.400030Z","shell.execute_reply.started":"2025-04-01T15:14:28.388558Z","shell.execute_reply":"2025-04-01T15:14:28.399229Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"df = pd.read_pickle(\"/kaggle/input/df.pkl\") \ndf = df.drop_duplicates(subset=['simplified_functions']).reset_index(drop = True)\ndf.head()","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:28.401221Z","iopub.execute_input":"2025-04-01T15:14:28.401452Z","iopub.status.idle":"2025-04-01T15:14:29.350621Z","shell.execute_reply.started":"2025-04-01T15:14:28.401432Z","shell.execute_reply":"2025-04-01T15:14:29.349730Z"}},"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"            function_tree     function                          taylor  \\\n0                     [x]            x                           1.0*x   \n1                [cos, x]       cos(x)  0.041667*x**4 - 0.5*x**2 + 1.0   \n2  [tan, sqrt, mul, x, x]       tan(x)            0.33333*x**3 + 1.0*x   \n3               [tanh, x]      tanh(x)           -0.33333*x**3 + 1.0*x   \n4       [add, tanh, x, x]  x + tanh(x)           -0.33333*x**3 + 2.0*x   \n\n  simplified_functions  \n0                    x  \n1               cos(x)  \n2               tan(x)  \n3              tanh(x)  \n4          x + tanh(x)  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_tree</th>\n      <th>function</th>\n      <th>taylor</th>\n      <th>simplified_functions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[x]</td>\n      <td>x</td>\n      <td>1.0*x</td>\n      <td>x</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[cos, x]</td>\n      <td>cos(x)</td>\n      <td>0.041667*x**4 - 0.5*x**2 + 1.0</td>\n      <td>cos(x)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[tan, sqrt, mul, x, x]</td>\n      <td>tan(x)</td>\n      <td>0.33333*x**3 + 1.0*x</td>\n      <td>tan(x)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[tanh, x]</td>\n      <td>tanh(x)</td>\n      <td>-0.33333*x**3 + 1.0*x</td>\n      <td>tanh(x)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[add, tanh, x, x]</td>\n      <td>x + tanh(x)</td>\n      <td>-0.33333*x**3 + 2.0*x</td>\n      <td>x + tanh(x)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":118},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self, precision=4, pos_dim=6, max_nums=50):\n        self.precision = precision\n\n        self.pos_dim = pos_dim\n\n        self.abs_embeddings, self.rel_embeddings = self.generate_dec_embeddings(max_nums)\n\n    def fit(self, functions):\n        self.enc_vocab = self.enc_load_vocab(functions)\n        self.dec_vocab, self.target_weights = self.dec_load_vocab()\n        self.enc_vocab_size = len(self.enc_vocab)\n        self.dec_vocab_size = len(self.dec_vocab)\n        self.enc_id_to_token = {idx: token for idx, token in enumerate(self.enc_vocab)}\n        self.dec_id_to_token = {idx: token for idx, token in enumerate(self.dec_vocab)}\n        self.enc_token_to_id = {token: idx for idx, token in enumerate(self.enc_vocab)}\n        self.dec_token_to_id = {token: idx for idx, token in enumerate(self.dec_vocab)}\n        print(\"Tokenizer fitted.\")\n        print(f\"Encoder vocab size: {self.enc_vocab_size}\")\n        print(f\"Decoder vocab size: {self.dec_vocab_size}\")\n\n    def dec_load_vocab(self):\n        \"\"\"\n        This is for loading the decoder vocab. While loading I also generate the weights for the tokens, since I use Weighted Cross Entropy\n        \"\"\"\n        vocab = []\n        weights = []\n\n        vocab += ['PAD', 'SOS', 'EOS']\n        weights += [0.0, 1.0, 1.0] \n\n        vocab += ['+', '-']\n        weights += [10] * 2\n\n        vocab += [f'{i}' for i in range(10)]\n        weights += [2] * 10\n\n        vocab += [f'E{i}' for i in range(-5, 6)]\n        weights += [7] * 11\n\n        vocab += [f'x{i}' for i in range(5)]\n        weights += [14] * 5\n\n        weight_tensor = torch.tensor(weights, dtype=torch.float32)\n\n        return vocab, weight_tensor\n\n    def return_dec_embeddings(self, seq_len):\n        abs_pos = self.abs_embeddings[:seq_len]\n        rel_pos = self.rel_embeddings[:seq_len, :seq_len]\n        return abs_pos, rel_pos\n\n    def generate_dec_embeddings(self, max_nums):\n        abs_pos = [0]\n        num_pos = [0]\n        for i in range(max_nums):\n            abs_pos.extend([i+1 for j in range(self.precision+3)])\n            num_pos.extend([1 - (j/(self.precision+3)) for j in range(self.precision+3)])\n        rel_pos = np.zeros((len(num_pos), len(num_pos)))\n        for i in range(len(num_pos)):\n            for j in range(len(num_pos)):\n                rel_pos[i][j] = num_pos[i] - num_pos[j]\n\n        return list(abs_pos), np.array(rel_pos)\n\n    def sympy_tokenizer(self, expr, path=None):\n        \"\"\"\n        Convert a SymPy expression to a tokenized prefix notation.\n        Returns tokens and paths (paths less relevant for basic LSTM).\n        \"\"\"\n        if path is None:\n            path = [1] \n\n        if expr.is_Number:\n\n            return [str(expr)], [path] \n\n        if expr.is_Symbol:\n            return [str(expr)], [path]\n\n        tokens = [expr.func.__name__.lower()]\n        paths = [path]\n\n        for i, arg in enumerate(expr.args):\n\n            new_tokens, new_paths = self.sympy_tokenizer(arg, path + [i])\n            tokens.extend(new_tokens)\n\n        return tokens, paths\n\n    def parse_token(self, token):\n        if token == 'exp1':\n            return False, 'e'\n        if '/' in token:\n            try:\n                frac = fractions.Fraction(token)\n                return True, float(frac)\n            except ValueError: \n                 return False, token\n        try:\n\n            return True, float(token)\n        except ValueError:\n            return False, token\n\n    def enc_load_vocab(self, functions):\n        vocab = ['PAD', '<UNK>'] \n        vocab += ['+', '-']\n        vocab += [f'{i}' for i in range(10)]\n        vocab += [f'E{i}' for i in range(-1, 2)] \n\n        all_tokens = set()\n        for fun in tqdm(functions, desc = \"Fitting Tokenizer (Encoder): \"):\n            try:\n                tokens, _ = self.sympy_tokenizer(fun)\n                for token in tokens:\n                    isNum, parsed_token = self.parse_token(token)\n                    if not isNum:\n                        all_tokens.add(parsed_token)\n            except Exception as e:\n                print(f\"Warning: Skipping function due to tokenization error: {fun} -> {e}\")\n                continue \n\n        vocab.extend(sorted(list(all_tokens)))\n        return vocab\n\n    def encode_number(self, x):\n        sign = '+' if x>=0 else '-'\n        x = abs(x)\n\n        if np.isclose(x, 0):\n\n             return ['+', 'E0'] + ['0'] * self.precision\n\n        sci_not = f\"{x:.{self.precision}e}\"\n        parts = sci_not.split('e')\n        mantissa_str = parts[0].replace('.', '').replace('-', '') \n        exp = int(parts[1])\n\n        effective_exp = exp\n\n        digits = list(mantissa_str.ljust(self.precision + 1, '0'))[:self.precision +1] \n\n        final_digits = digits[1:self.precision+1] \n\n        seq = [sign, f'E{effective_exp}', digits[0]] \n        seq.extend(final_digits) \n\n        if len(seq) > self.precision + 2:\n             seq = seq[:self.precision+2]\n        elif len(seq) < self.precision + 2:\n\n             seq.extend(['0'] * (self.precision + 2 - len(seq)))\n\n        exp_val = effective_exp\n        if exp_val < -5: exp_val = -5\n        if exp_val > 5: exp_val = 5\n        seq[1] = f'E{exp_val}' \n\n        return seq\n\n    def decode_number(self, x):\n        \"\"\"\n        This function is for getting back a float number using its P10 tokenization\n        \"\"\"\n        if not x or len(x) < self.precision + 2: \n             return 0.0\n\n        sign_token = x[0]\n        exp_token = x[1]\n        digit_tokens = x[2:]\n\n        sign = -1 if sign_token == '-' else 1\n        try:\n            exp = int(exp_token[1:]) \n        except (ValueError, IndexError):\n            print(f\"Warning: Invalid exponent token: {exp_token}\")\n            return 0.0 \n\n        digit_tokens = list(digit_tokens) \n        while len(digit_tokens) < self.precision:\n            digit_tokens.append('0')\n        digit_tokens = digit_tokens[:self.precision] \n\n        num_str = digit_tokens[0] + \".\" + \"\".join(digit_tokens[1:])\n\n        try:\n            num = float(num_str) * (10**exp)\n        except ValueError:\n            print(f\"Warning: Invalid number format during decode: {num_str}\")\n            return 0.0\n\n        num *= sign\n\n        return num\n\n    def encode_dec(self, poly):\n        \"\"\"\n        Tokenizes the output polynomial sequence for the LSTM decoder.\n        Returns only the token IDs.\n        \"\"\"\n        variables = list(poly.free_symbols)\n\n        if not variables:\n\n             x = sp.symbols('x') \n             coeffs = [float(poly)] + [0.0] * 4 \n        elif len(variables) > 1:\n             print(f\"Warning: Multiple variables found in polynomial: {variables}. Using {variables[0]}.\")\n             x = variables[0]\n\n             try:\n                 coeff_dict = poly.as_coefficients_dict()\n                 coeffs = [float(coeff_dict.get(x**i, 0.0)) for i in range(5)]\n             except Exception as e:\n                 print(f\"Error extracting coefficients for multi-variable poly: {poly} -> {e}\")\n                 coeffs = [0.0] * 5 \n        else:\n             x = variables[0]\n             coeff_dict = poly.as_coefficients_dict()\n             coeffs = [float(coeff_dict.get(x**i, 0.0)) for i in range(5)]\n\n        seq = ['SOS']\n        added_term = False\n        for i, coeff in enumerate(coeffs):\n\n            if np.isclose(coeff, 0.0, atol=1e-9): \n                continue\n            seq.append(f'x{i}')\n            seq.extend(self.encode_number(coeff))\n            added_term = True\n\n        if not added_term:\n             seq.append('x0')\n             seq.extend(self.encode_number(0.0))\n\n        seq.append('EOS')\n\n        token_ids = []\n        for token in seq:\n            token_id = self.dec_token_to_id.get(token)\n            if token_id is None:\n                print(f\"Warning: Token '{token}' not found in decoder vocab during encode_dec. Using PAD.\")\n                token_ids.append(self.dec_token_to_id['PAD']) \n            else:\n                token_ids.append(token_id)\n\n        return token_ids \n\n    def decode_dec(self, seq_ids):\n        \"\"\"\n        Converts a sequence of decoder token IDs back to tokens.\n        \"\"\"\n        return [self.dec_id_to_token.get(id, '<UNK>') for id in seq_ids]\n\n    def seq_to_coeffs(self, seq_tokens):\n        \"\"\"\n        Converts a sequence of decoded tokens (strings) back to coefficient list.\n        \"\"\"\n        coeffs = [0.0] * 5 \n        num_list = []\n        current_degree = -1 \n\n        if not seq_tokens or seq_tokens[0] != 'SOS':\n             print(f\"Warning: Sequence does not start with SOS: {seq_tokens[:5]}\")\n\n        seq_iter = iter(seq_tokens)\n        if seq_tokens[0] == 'SOS':\n            next(seq_iter) \n\n        for token in seq_iter:\n            if token == 'EOS':\n\n                if current_degree != -1 and num_list:\n                    num = self.decode_number(num_list)\n                    if 0 <= current_degree < 5:\n                        coeffs[current_degree] = num \n                    else:\n                        print(f\"Warning: Invalid degree {current_degree} encountered before EOS.\")\n                break \n\n            elif token.startswith('x') and token[1:].isdigit():\n\n                if current_degree != -1 and num_list:\n                     num = self.decode_number(num_list)\n                     if 0 <= current_degree < 5:\n                         coeffs[current_degree] = num \n                     else:\n                         print(f\"Warning: Invalid degree {current_degree} encountered.\")\n\n                num_list = []\n                try:\n                    current_degree = int(token[1:])\n                except ValueError:\n                     print(f\"Warning: Invalid degree token {token}\")\n                     current_degree = -1 \n\n            elif token in ['+', '-'] or token.startswith('E') or token.isdigit():\n\n                 if current_degree != -1: \n                     num_list.append(token)\n                 else:\n\n                     pass \n\n            elif token not in ['SOS']: \n                 print(f\"Warning: Unexpected token '{token}' during coefficient parsing.\")\n\n                 num_list = []\n\n        if current_degree != -1 and num_list and token != 'EOS':\n             num = self.decode_number(num_list)\n             if 0 <= current_degree < 5:\n                 coeffs[current_degree] = num\n             else:\n                 print(f\"Warning: Invalid degree {current_degree} encountered at end of sequence.\")\n\n        coeffs = [round(c, self.precision) for c in coeffs]\n\n        return coeffs\n\n    def encode_enc(self, fun):\n        \"\"\"\n        Tokenizes the input function for the LSTM encoder.\n        Returns only the sequence of token IDs.\n        \"\"\"\n        try:\n            tokens, _ = self.sympy_tokenizer(fun) \n        except Exception as e:\n             print(f\"Error tokenizing function {fun}: {e}\")\n\n             return [self.enc_token_to_id.get('PAD', 0)] \n\n        seq = []\n        for token in tokens:\n            isNum, parsed_token = self.parse_token(token)\n            if isNum:\n\n                num_tokens = self.encode_number(parsed_token)\n                seq.extend(num_tokens)\n            else:\n\n                 processed_token = str(parsed_token).lower() \n                 seq.append(processed_token)\n\n        token_ids = []\n        for token in seq:\n             token_id = self.enc_token_to_id.get(token, self.enc_token_to_id.get('<UNK>'))\n             if token_id is None: \n                 print(f\"Critical Error: <UNK> token not found in encoder vocab!\")\n                 token_id = 0 \n             token_ids.append(token_id)\n\n        return token_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:14:29.351981Z","iopub.execute_input":"2025-04-01T15:14:29.352312Z","iopub.status.idle":"2025-04-01T15:14:29.382263Z","shell.execute_reply.started":"2025-04-01T15:14:29.352279Z","shell.execute_reply":"2025-04-01T15:14:29.381296Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TaylorDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super().__init__()\n        self.functions = df['simplified_functions'].to_list()\n        self.polynomials = df['taylor'].to_list()\n        self.tokenizer = tokenizer\n        self.enc_vocab_size = self.tokenizer.enc_vocab_size\n        self.dec_vocab_size = self.tokenizer.dec_vocab_size\n\n    def __len__(self):\n        return len(self.functions)\n\n    def __getitem__(self, idx):\n        fun = self.functions[idx]\n        poly = self.polynomials[idx]\n\n        enc_seq_ids = self.tokenizer.encode_enc(fun)\n        out_seq_ids = self.tokenizer.encode_dec(poly)\n\n        enc_seq = torch.tensor(enc_seq_ids, dtype=torch.long)\n        out_seq = torch.tensor(out_seq_ids, dtype=torch.long)\n\n        return {\n            'inputs': enc_seq,\n            'outputs': out_seq\n        }","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.384131Z","iopub.execute_input":"2025-04-01T15:14:29.384406Z","iopub.status.idle":"2025-04-01T15:14:29.399180Z","shell.execute_reply.started":"2025-04-01T15:14:29.384384Z","shell.execute_reply":"2025-04-01T15:14:29.398489Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Custom collate_fn for LSTM. Pads only the input and output sequences.\n    \"\"\"\n    enc_seqs = []\n    out_seqs = []\n    enc_lens = []\n\n    for item in batch:\n        enc_seqs.append(item['inputs'])\n        out_seqs.append(item['outputs'])\n        enc_lens.append(len(item['inputs']))\n\n    enc_pad_value = 0 \n    dec_pad_value = 0 \n\n    enc_seqs_padded = pad_sequence(enc_seqs, batch_first=True, padding_value=enc_pad_value)\n    out_seqs_padded = pad_sequence(out_seqs, batch_first=True, padding_value=dec_pad_value)\n\n    return {\n        'inputs': enc_seqs_padded,\n        'input_lengths': torch.tensor(enc_lens),\n        'outputs': out_seqs_padded\n    }","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.400426Z","iopub.execute_input":"2025-04-01T15:14:29.400685Z","iopub.status.idle":"2025-04-01T15:14:29.418002Z","shell.execute_reply.started":"2025-04-01T15:14:29.400663Z","shell.execute_reply":"2025-04-01T15:14:29.417205Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"class EncoderLSTM(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src, src_len):\n\n        embedded = self.dropout(self.embedding(src))\n\n        packed_embedded = pack_padded_sequence(embedded, src_len.to('cpu'), batch_first=True, enforce_sorted=False)\n\n        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n\n        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n\n        return outputs, hidden, cell","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.418731Z","iopub.execute_input":"2025-04-01T15:14:29.419054Z","iopub.status.idle":"2025-04-01T15:14:29.431981Z","shell.execute_reply.started":"2025-04-01T15:14:29.419011Z","shell.execute_reply":"2025-04-01T15:14:29.431253Z"}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        self.attn = nn.Linear((enc_hid_dim) + dec_hid_dim, dec_hid_dim, bias=False) \n        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n\n    def forward(self, hidden, encoder_outputs, mask):\n\n        batch_size = encoder_outputs.shape[0]\n        src_len = encoder_outputs.shape[1]\n\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n\n        attention = self.v(energy).squeeze(2)\n\n        attention = attention.masked_fill(mask == 0, -1e10) \n\n        return F.softmax(attention, dim=1)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.432666Z","iopub.execute_input":"2025-04-01T15:14:29.432929Z","iopub.status.idle":"2025-04-01T15:14:29.449468Z","shell.execute_reply.started":"2025-04-01T15:14:29.432885Z","shell.execute_reply":"2025-04-01T15:14:29.448644Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"class DecoderLSTM(nn.Module):\n    def __init__(self, output_dim, embed_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, embed_dim)\n\n        self.rnn = nn.LSTM(enc_hid_dim + embed_dim, dec_hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(enc_hid_dim + dec_hid_dim + embed_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell, encoder_outputs, mask):\n\n        input = input.unsqueeze(1) \n\n        embedded = self.dropout(self.embedding(input))\n\n        a = self.attention(hidden[-1], encoder_outputs, mask)\n\n        a = a.unsqueeze(1)\n\n        weighted = torch.bmm(a, encoder_outputs)\n\n        rnn_input = torch.cat((embedded, weighted), dim=2)\n\n        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n\n        embedded = embedded.squeeze(1)\n        output = output.squeeze(1)\n        weighted = weighted.squeeze(1)\n\n        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n\n        return prediction, hidden, cell","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.450326Z","iopub.execute_input":"2025-04-01T15:14:29.450600Z","iopub.status.idle":"2025-04-01T15:14:29.463875Z","shell.execute_reply.started":"2025-04-01T15:14:29.450571Z","shell.execute_reply":"2025-04-01T15:14:29.463170Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"class LSTMSeq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n        self.enc_vocab_size = encoder.embedding.num_embeddings\n        self.dec_vocab_size = decoder.output_dim\n\n    def create_mask(self, src):\n\n         mask = (src != 0)\n         return mask \n\n    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        encoder_outputs, hidden, cell = self.encoder(src, src_len)\n\n        input = trg[:, 0]\n\n        mask = self.create_mask(src) \n\n        for t in range(1, trg_len): \n\n            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs, mask)\n\n            outputs[:, t] = output\n\n            teacher_force = random.random() < teacher_forcing_ratio\n\n            top1 = output.argmax(1)\n\n            input = trg[:, t] if teacher_force else top1\n\n        return outputs","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.465951Z","iopub.execute_input":"2025-04-01T15:14:29.466168Z","iopub.status.idle":"2025-04-01T15:14:29.485066Z","shell.execute_reply.started":"2025-04-01T15:14:29.466148Z","shell.execute_reply":"2025-04-01T15:14:29.484396Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, scheduler, clip=1.0): \n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(dataloader, desc='Training', leave=False):\n        src = batch['inputs'].to(device)\n        trg = batch['outputs'].to(device)\n        src_len = batch['input_lengths'].to(device) \n\n        optimizer.zero_grad()\n\n        output = model(src, src_len, trg[:, :-1], teacher_forcing_ratio=0.5) \n\n        output_dim = output.shape[-1]\n        output = output.reshape(-1, output_dim) \n        targets = trg[:, 1:].reshape(-1)        \n\n        loss = criterion(output, targets)\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) \n        optimizer.step()\n        if scheduler: \n             scheduler.step() \n\n    return total_loss / len(dataloader)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.486280Z","iopub.execute_input":"2025-04-01T15:14:29.486488Z","iopub.status.idle":"2025-04-01T15:14:29.500727Z","shell.execute_reply.started":"2025-04-01T15:14:29.486469Z","shell.execute_reply":"2025-04-01T15:14:29.499978Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"def validate(model, dataloader, criterion):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc='Validation', leave=False):\n            src = batch['inputs'].to(device)\n            trg = batch['outputs'].to(device)\n            src_len = batch['input_lengths'].to(device) # Get lengths\n\n             # Turn off teacher forcing for validation\n            output = model(src, src_len, trg[:,:-1], teacher_forcing_ratio=0.0)\n\n            output_dim = output.shape[-1]\n            output = output.reshape(-1, output_dim)\n            targets = trg[:, 1:].reshape(-1)\n\n            loss = criterion(output, targets)\n            total_loss += loss.item()\n\n        return total_loss / len(dataloader)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.501535Z","iopub.execute_input":"2025-04-01T15:14:29.501793Z","iopub.status.idle":"2025-04-01T15:14:29.513466Z","shell.execute_reply.started":"2025-04-01T15:14:29.501760Z","shell.execute_reply":"2025-04-01T15:14:29.512893Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"def finite_state_prediction(logits, state, tokenizer): \n\n    logits = logits.squeeze().reshape(1, -1)\n\n    state_dict = ['number', 'degree', 'exponent', 'sign']\n    assert state in state_dict, \"state not in predefined states\"\n\n    number_states = [idx for idx, val in enumerate(tokenizer.dec_vocab) if val in '1234567890']\n    degree_states = [idx for idx, val in enumerate(tokenizer.dec_vocab) if val.startswith('x') and val[1:].isdigit() or val == 'EOS']\n    sign_states = [idx for idx, val in enumerate(tokenizer.dec_vocab) if val in '+-']\n    exponent_states = [idx for idx, val in enumerate(tokenizer.dec_vocab) if val.startswith('E')]\n\n    state_indices = {\n        'number': number_states,\n        'degree': degree_states,\n        'exponent': exponent_states,\n        'sign': sign_states\n    }\n\n    valid_indices = state_indices[state]\n\n    if not valid_indices:\n        return torch.tensor(tokenizer.dec_token_to_id['EOS']).to(logits.device)\n\n    state_logits = logits[0, valid_indices]\n\n    best_local_idx = torch.argmax(state_logits).item()\n\n    max_idx = valid_indices[best_local_idx]\n\n    return torch.tensor(max_idx).to(logits.device)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.514145Z","iopub.execute_input":"2025-04-01T15:14:29.514380Z","iopub.status.idle":"2025-04-01T15:14:29.535152Z","shell.execute_reply.started":"2025-04-01T15:14:29.514347Z","shell.execute_reply":"2025-04-01T15:14:29.534421Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"def evaluate_model(model, df, tokenizer):\n    model.eval()\n    preds = []\n    targets = []\n\n    max_len = 1 + 5 * (3 + tokenizer.precision) + 1 \n\n    sos_idx = tokenizer.dec_token_to_id['SOS']\n    eos_idx = tokenizer.dec_token_to_id['EOS']\n\n    with torch.no_grad():\n        for i, row in tqdm(df.iterrows(), desc=\"Generating Predictions: \", total=df.shape[0]):\n            function = row['simplified_functions']\n            polynomial = row['taylor']\n\n            src_tokens = tokenizer.encode_enc(function)\n            src_tensor = torch.tensor(src_tokens).unsqueeze(0).to(device)\n            src_len = torch.tensor([len(src_tokens)]).to(device)\n\n            target_tokens = tokenizer.encode_dec(polynomial)\n            targets.append(tokenizer.seq_to_coeffs(tokenizer.decode_dec(target_tokens))) \n\n            trg_indexes = [sos_idx]\n            cur_state = 'degree' \n            next_state_in = 1 \n\n            encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n            mask = model.create_mask(src_tensor) \n\n            for _ in range(max_len):\n                trg_tensor = torch.tensor([trg_indexes[-1]]).to(device) \n\n                output, hidden, cell = model.decoder(trg_tensor, hidden, cell, encoder_outputs, mask)\n\n                pred_token = finite_state_prediction(output, cur_state, tokenizer).item() \n\n                trg_indexes.append(pred_token)\n\n                next_state_in -= 1\n                if next_state_in <= 0:\n\n                    predicted_token_str = tokenizer.dec_id_to_token.get(pred_token, '')\n                    if predicted_token_str.startswith('x') or predicted_token_str == 'EOS':\n                         cur_state = 'sign'\n                         next_state_in = 1\n                    elif predicted_token_str in ['+', '-']:\n                         cur_state = 'exponent'\n                         next_state_in = 1\n                    elif predicted_token_str.startswith('E'):\n                         cur_state = 'number'\n                         next_state_in = tokenizer.precision + 1 \n                    elif predicted_token_str.isdigit():\n\n                         num_digits_so_far = sum(1 for tok_idx in trg_indexes \\\n                                                 if tokenizer.dec_id_to_token.get(tok_idx, '').isdigit())\n                         exp_idx = -1\n                         for k, tok_idx in enumerate(reversed(trg_indexes)):\n                              if tokenizer.dec_id_to_token.get(tok_idx, '').startswith('E'):\n                                  exp_idx = len(trg_indexes) - 1 - k\n                                  break\n                         digits_after_exp = sum(1 for k, tok_idx in enumerate(trg_indexes) if k > exp_idx and \\\n                                                  tokenizer.dec_id_to_token.get(tok_idx, '').isdigit())\n\n                         if digits_after_exp >= tokenizer.precision + 1: \n                              cur_state = 'degree'\n                              next_state_in = 1\n                         else:\n                              cur_state = 'number' \n                              next_state_in = 1 \n\n                    else: \n\n                         cur_state = 'degree'\n                         next_state_in = 1\n\n                if pred_token == eos_idx:\n                    break\n\n            pred_decoded_tokens = tokenizer.decode_dec(trg_indexes)\n            pred_coeffs = tokenizer.seq_to_coeffs(pred_decoded_tokens)\n            preds.append(pred_coeffs)\n\n    return preds, targets","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.535902Z","iopub.execute_input":"2025-04-01T15:14:29.536093Z","iopub.status.idle":"2025-04-01T15:14:29.550645Z","shell.execute_reply.started":"2025-04-01T15:14:29.536075Z","shell.execute_reply":"2025-04-01T15:14:29.549862Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"def polynomial_rmse(preds, targets, n=100, x_range=(-1, 1)):\n    rmse_sum_sq = 0.0\n    count = 0\n\n    for pred, target in zip(preds, targets):\n\n        if not isinstance(pred, (list, np.ndarray)) or not isinstance(target, (list, np.ndarray)):\n            continue\n        if not all(isinstance(x, (int, float, np.number)) for x in pred) or \\\n           not all(isinstance(x, (int, float, np.number)) for x in target):\n            continue\n\n        x = sp.symbols('x')\n        try:\n            pred_poly = sum(float(coef) * x**i for i, coef in enumerate(pred))\n            target_poly = sum(float(coef) * x**i for i, coef in enumerate(target))\n        except (TypeError, ValueError) as e:\n            continue\n\n        x_vals = np.linspace(x_range[0], x_range[1], n)\n\n        try:\n            pred_func = sp.lambdify(x, pred_poly, 'numpy')\n            target_func = sp.lambdify(x, target_poly, 'numpy')\n\n            y_pred = pred_func(x_vals)\n            y_true = target_func(x_vals)\n\n            if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)) or \\\n               np.any(np.isnan(y_true)) or np.any(np.isinf(y_true)):\n                 continue\n\n            rmse_sum_sq += mean_squared_error(y_true, y_pred)\n            count += 1\n        except Exception as e:\n            continue\n\n    if count == 0:\n        print(\"Warning: No valid pairs processed for polynomial RMSE.\")\n        return float('nan')\n\n    mean_rmse_sq = rmse_sum_sq / count\n    return np.sqrt(mean_rmse_sq)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.551277Z","iopub.execute_input":"2025-04-01T15:14:29.551461Z","iopub.status.idle":"2025-04-01T15:14:29.567900Z","shell.execute_reply.started":"2025-04-01T15:14:29.551445Z","shell.execute_reply":"2025-04-01T15:14:29.567124Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"def coeff_rmse(preds, targets):\n    rmse_sum_sq = 0.0\n    count = 0\n    for pred, target in zip(preds, targets):\n\n        if not isinstance(pred, (list, np.ndarray)) or not isinstance(target, (list, np.ndarray)) or len(pred) != len(target):\n            continue\n        if not all(isinstance(x, (int, float, np.number)) for x in pred) or \\\n           not all(isinstance(x, (int, float, np.number)) for x in target):\n            continue\n\n        try:\n\n             pred_arr = np.array(pred, dtype=float)\n             target_arr = np.array(target, dtype=float)\n\n             if np.any(np.isnan(pred_arr)) or np.any(np.isinf(pred_arr)) or \\\n                np.any(np.isnan(target_arr)) or np.any(np.isinf(target_arr)):\n                  continue\n\n             rmse_sum_sq += mean_squared_error(target_arr, pred_arr)\n             count += 1\n        except Exception as e:\n             continue\n\n    if count == 0:\n        print(\"Warning: No valid pairs processed for coefficient RMSE.\")\n        return float('nan')\n\n    mean_rmse_sq = rmse_sum_sq / count\n    return np.sqrt(mean_rmse_sq)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.568878Z","iopub.execute_input":"2025-04-01T15:14:29.569156Z","iopub.status.idle":"2025-04-01T15:14:29.586370Z","shell.execute_reply.started":"2025-04-01T15:14:29.569127Z","shell.execute_reply":"2025-04-01T15:14:29.585438Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"tokenizer = Tokenizer(precision=4) \ntokenizer.fit(df['simplified_functions'])","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.587110Z","iopub.execute_input":"2025-04-01T15:14:29.587316Z","iopub.status.idle":"2025-04-01T15:14:29.723501Z","shell.execute_reply.started":"2025-04-01T15:14:29.587298Z","shell.execute_reply":"2025-04-01T15:14:29.722543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fitting Tokenizer (Encoder):   0%|          | 0/2521 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b6b5d5c9cef434b92e73ff1de3abe9a"}},"metadata":{}},{"name":"stdout","text":"Tokenizer fitted.\nEncoder vocab size: 37\nDecoder vocab size: 31\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, train_size=0.9, random_state=seed)\ntrain_df, val_df = train_test_split(train_df, train_size=0.8, random_state=seed)\n\ntrain_data = TaylorDataset(train_df, tokenizer)\nval_data = TaylorDataset(val_df, tokenizer)\n\ntrain_load = DataLoader(train_data, shuffle=True, batch_size=64, collate_fn=collate_fn, num_workers=2, pin_memory=True)\nval_load = DataLoader(val_data, shuffle=False, batch_size=64, collate_fn=collate_fn, num_workers=2, pin_memory=True)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.724337Z","iopub.execute_input":"2025-04-01T15:14:29.724605Z","iopub.status.idle":"2025-04-01T15:14:29.734032Z","shell.execute_reply.started":"2025-04-01T15:14:29.724583Z","shell.execute_reply":"2025-04-01T15:14:29.733201Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"ENC_EMBED_DIM = 256\nDEC_EMBED_DIM = 256\nHID_DIM = 512 \nN_LAYERS = 2\nENC_DROPOUT = 0.3\nDEC_DROPOUT = 0.3\nLEARNING_RATE = 1e-4 \nWEIGHT_DECAY = 0.01\nCLIP = 1.0 \n\nattn = Attention(HID_DIM, HID_DIM)\nenc = EncoderLSTM(tokenizer.enc_vocab_size, ENC_EMBED_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = DecoderLSTM(tokenizer.dec_vocab_size, DEC_EMBED_DIM, HID_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n\nmodel = LSTMSeq2Seq(enc, dec, device).to(device)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.734926Z","iopub.execute_input":"2025-04-01T15:14:29.735163Z","iopub.status.idle":"2025-04-01T15:14:29.815917Z","shell.execute_reply.started":"2025-04-01T15:14:29.735143Z","shell.execute_reply":"2025-04-01T15:14:29.815023Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"def init_weights(m):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\nprint(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.816833Z","iopub.execute_input":"2025-04-01T15:14:29.817130Z","iopub.status.idle":"2025-04-01T15:14:29.822449Z","shell.execute_reply.started":"2025-04-01T15:14:29.817101Z","shell.execute_reply":"2025-04-01T15:14:29.821767Z"}},"outputs":[{"name":"stdout","text":"The model has 8,986,911 trainable parameters\n","output_type":"stream"}],"execution_count":135},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.dec_token_to_id['PAD'],\n                               weight=tokenizer.target_weights.to(device))\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) ","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.823200Z","iopub.execute_input":"2025-04-01T15:14:29.823484Z","iopub.status.idle":"2025-04-01T15:14:29.879031Z","shell.execute_reply.started":"2025-04-01T15:14:29.823455Z","shell.execute_reply":"2025-04-01T15:14:29.878303Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"num_epochs = 100\nbest_val_loss = float('inf')\nsave_path = 'best_lstm_model.pth' \npatience = 10\nno_improve_epochs = 0\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    train_loss = train_one_epoch(model, train_load, criterion, optimizer, scheduler, CLIP) \n    val_loss = validate(model, val_load, criterion)\n\n    if scheduler:\n         scheduler.step()\n\n    print(f\"Epoch {epoch+1:02}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), save_path)\n        print(f\"*** New best model saved with validation loss: {best_val_loss:.4f} ***\")\n        no_improve_epochs = 0\n    else:\n        no_improve_epochs += 1\n        print(f\"Validation loss did not improve for {no_improve_epochs} epoch(s).\")\n        if no_improve_epochs >= patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs!\")\n            break\n    print(\"-\" * 50)\n\nprint(\"Training finished.\")","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:14:29.879903Z","iopub.execute_input":"2025-04-01T15:14:29.880141Z","iopub.status.idle":"2025-04-01T15:20:45.495172Z","shell.execute_reply.started":"2025-04-01T15:14:29.880120Z","shell.execute_reply":"2025-04-01T15:20:45.494099Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 01/100 - Train Loss: 3.1519 - Val Loss: 2.7497 - LR: 0.000035\n*** New best model saved with validation loss: 2.7497 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02/100 - Train Loss: 2.6835 - Val Loss: 2.6911 - LR: 0.000010\n*** New best model saved with validation loss: 2.6911 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03/100 - Train Loss: 2.5821 - Val Loss: 2.4619 - LR: 0.000090\n*** New best model saved with validation loss: 2.4619 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04/100 - Train Loss: 2.2988 - Val Loss: 2.3905 - LR: 0.000065\n*** New best model saved with validation loss: 2.3905 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05/100 - Train Loss: 2.0531 - Val Loss: 2.5977 - LR: 0.000000\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 06/100 - Train Loss: 1.9926 - Val Loss: 2.8431 - LR: 0.000065\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 07/100 - Train Loss: 1.7826 - Val Loss: 2.0543 - LR: 0.000090\n*** New best model saved with validation loss: 2.0543 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 08/100 - Train Loss: 1.5635 - Val Loss: 1.9893 - LR: 0.000010\n*** New best model saved with validation loss: 1.9893 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 09/100 - Train Loss: 1.5080 - Val Loss: 2.2965 - LR: 0.000035\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/100 - Train Loss: 1.5021 - Val Loss: 2.5327 - LR: 0.000100\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/100 - Train Loss: 1.4170 - Val Loss: 1.9989 - LR: 0.000035\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/100 - Train Loss: 1.3731 - Val Loss: 1.4919 - LR: 0.000010\n*** New best model saved with validation loss: 1.4919 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/100 - Train Loss: 1.3650 - Val Loss: 1.4815 - LR: 0.000090\n*** New best model saved with validation loss: 1.4815 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/100 - Train Loss: 1.3286 - Val Loss: 1.3641 - LR: 0.000065\n*** New best model saved with validation loss: 1.3641 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/100 - Train Loss: 1.2974 - Val Loss: 1.3591 - LR: 0.000000\n*** New best model saved with validation loss: 1.3591 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/100 - Train Loss: 1.2828 - Val Loss: 1.3545 - LR: 0.000065\n*** New best model saved with validation loss: 1.3545 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/100 - Train Loss: 1.2681 - Val Loss: 1.3081 - LR: 0.000090\n*** New best model saved with validation loss: 1.3081 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/100 - Train Loss: 1.2421 - Val Loss: 1.2954 - LR: 0.000010\n*** New best model saved with validation loss: 1.2954 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/100 - Train Loss: 1.2253 - Val Loss: 1.2898 - LR: 0.000035\n*** New best model saved with validation loss: 1.2898 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/100 - Train Loss: 1.2184 - Val Loss: 1.2862 - LR: 0.000100\n*** New best model saved with validation loss: 1.2862 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 21/100 - Train Loss: 1.2159 - Val Loss: 1.2794 - LR: 0.000035\n*** New best model saved with validation loss: 1.2794 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 22/100 - Train Loss: 1.2022 - Val Loss: 1.2594 - LR: 0.000010\n*** New best model saved with validation loss: 1.2594 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 23/100 - Train Loss: 1.1911 - Val Loss: 1.2566 - LR: 0.000090\n*** New best model saved with validation loss: 1.2566 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 24/100 - Train Loss: 1.1875 - Val Loss: 1.2535 - LR: 0.000065\n*** New best model saved with validation loss: 1.2535 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 25/100 - Train Loss: 1.1703 - Val Loss: 1.2421 - LR: 0.000000\n*** New best model saved with validation loss: 1.2421 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 26/100 - Train Loss: 1.1698 - Val Loss: 1.2340 - LR: 0.000065\n*** New best model saved with validation loss: 1.2340 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 27/100 - Train Loss: 1.1525 - Val Loss: 1.2279 - LR: 0.000090\n*** New best model saved with validation loss: 1.2279 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 28/100 - Train Loss: 1.1559 - Val Loss: 1.2237 - LR: 0.000010\n*** New best model saved with validation loss: 1.2237 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 29/100 - Train Loss: 1.1533 - Val Loss: 1.2136 - LR: 0.000035\n*** New best model saved with validation loss: 1.2136 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 30/100 - Train Loss: 1.1479 - Val Loss: 1.2356 - LR: 0.000100\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 31/100 - Train Loss: 1.1488 - Val Loss: 1.2183 - LR: 0.000035\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 32/100 - Train Loss: 1.1265 - Val Loss: 1.2024 - LR: 0.000010\n*** New best model saved with validation loss: 1.2024 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 33/100 - Train Loss: 1.1281 - Val Loss: 1.2271 - LR: 0.000090\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 34/100 - Train Loss: 1.1295 - Val Loss: 1.2046 - LR: 0.000065\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 35/100 - Train Loss: 1.1170 - Val Loss: 1.1924 - LR: 0.000000\n*** New best model saved with validation loss: 1.1924 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 36/100 - Train Loss: 1.1064 - Val Loss: 1.1997 - LR: 0.000065\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 37/100 - Train Loss: 1.1119 - Val Loss: 1.1929 - LR: 0.000090\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 38/100 - Train Loss: 1.1063 - Val Loss: 1.1797 - LR: 0.000010\n*** New best model saved with validation loss: 1.1797 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 39/100 - Train Loss: 1.0913 - Val Loss: 1.1847 - LR: 0.000035\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 40/100 - Train Loss: 1.1001 - Val Loss: 1.2336 - LR: 0.000100\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 41/100 - Train Loss: 1.0995 - Val Loss: 1.1793 - LR: 0.000035\n*** New best model saved with validation loss: 1.1793 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 42/100 - Train Loss: 1.0831 - Val Loss: 1.1784 - LR: 0.000010\n*** New best model saved with validation loss: 1.1784 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 43/100 - Train Loss: 1.0823 - Val Loss: 1.1874 - LR: 0.000090\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 44/100 - Train Loss: 1.0869 - Val Loss: 1.1723 - LR: 0.000065\n*** New best model saved with validation loss: 1.1723 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 45/100 - Train Loss: 1.0765 - Val Loss: 1.1621 - LR: 0.000000\n*** New best model saved with validation loss: 1.1621 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 46/100 - Train Loss: 1.0661 - Val Loss: 1.1689 - LR: 0.000065\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 47/100 - Train Loss: 1.0667 - Val Loss: 1.1652 - LR: 0.000090\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 48/100 - Train Loss: 1.0642 - Val Loss: 1.1646 - LR: 0.000010\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 49/100 - Train Loss: 1.0507 - Val Loss: 1.1528 - LR: 0.000035\n*** New best model saved with validation loss: 1.1528 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 50/100 - Train Loss: 1.0570 - Val Loss: 1.1875 - LR: 0.000100\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 51/100 - Train Loss: 1.0619 - Val Loss: 1.1575 - LR: 0.000035\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 52/100 - Train Loss: 1.0370 - Val Loss: 1.1576 - LR: 0.000010\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 53/100 - Train Loss: 1.0395 - Val Loss: 1.1674 - LR: 0.000090\nValidation loss did not improve for 4 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 54/100 - Train Loss: 1.0448 - Val Loss: 1.1784 - LR: 0.000065\nValidation loss did not improve for 5 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 55/100 - Train Loss: 1.0326 - Val Loss: 1.1615 - LR: 0.000000\nValidation loss did not improve for 6 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 56/100 - Train Loss: 1.0237 - Val Loss: 1.1413 - LR: 0.000065\n*** New best model saved with validation loss: 1.1413 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 57/100 - Train Loss: 1.0309 - Val Loss: 1.1477 - LR: 0.000090\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 58/100 - Train Loss: 1.0264 - Val Loss: 1.1443 - LR: 0.000010\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 59/100 - Train Loss: 1.0199 - Val Loss: 1.1554 - LR: 0.000035\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 60/100 - Train Loss: 1.0170 - Val Loss: 1.1657 - LR: 0.000100\nValidation loss did not improve for 4 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 61/100 - Train Loss: 1.0201 - Val Loss: 1.1809 - LR: 0.000035\nValidation loss did not improve for 5 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 62/100 - Train Loss: 0.9996 - Val Loss: 1.1453 - LR: 0.000010\nValidation loss did not improve for 6 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 63/100 - Train Loss: 1.0067 - Val Loss: 1.1588 - LR: 0.000090\nValidation loss did not improve for 7 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 64/100 - Train Loss: 1.0141 - Val Loss: 1.1787 - LR: 0.000065\nValidation loss did not improve for 8 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 65/100 - Train Loss: 1.0020 - Val Loss: 1.1356 - LR: 0.000000\n*** New best model saved with validation loss: 1.1356 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 66/100 - Train Loss: 0.9808 - Val Loss: 1.1362 - LR: 0.000065\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 67/100 - Train Loss: 0.9896 - Val Loss: 1.1352 - LR: 0.000090\n*** New best model saved with validation loss: 1.1352 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 68/100 - Train Loss: 0.9923 - Val Loss: 1.1268 - LR: 0.000010\n*** New best model saved with validation loss: 1.1268 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 69/100 - Train Loss: 0.9684 - Val Loss: 1.1358 - LR: 0.000035\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 70/100 - Train Loss: 0.9861 - Val Loss: 1.1278 - LR: 0.000100\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 71/100 - Train Loss: 0.9924 - Val Loss: 1.1491 - LR: 0.000035\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 72/100 - Train Loss: 0.9698 - Val Loss: 1.1270 - LR: 0.000010\nValidation loss did not improve for 4 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 73/100 - Train Loss: 0.9724 - Val Loss: 1.1751 - LR: 0.000090\nValidation loss did not improve for 5 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 74/100 - Train Loss: 0.9831 - Val Loss: 1.1208 - LR: 0.000065\n*** New best model saved with validation loss: 1.1208 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 75/100 - Train Loss: 0.9651 - Val Loss: 1.1240 - LR: 0.000000\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 76/100 - Train Loss: 0.9549 - Val Loss: 1.1499 - LR: 0.000065\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 77/100 - Train Loss: 0.9604 - Val Loss: 1.1437 - LR: 0.000090\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 78/100 - Train Loss: 0.9596 - Val Loss: 1.1264 - LR: 0.000010\nValidation loss did not improve for 4 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 79/100 - Train Loss: 0.9447 - Val Loss: 1.1267 - LR: 0.000035\nValidation loss did not improve for 5 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 80/100 - Train Loss: 0.9424 - Val Loss: 1.1495 - LR: 0.000100\nValidation loss did not improve for 6 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 81/100 - Train Loss: 0.9504 - Val Loss: 1.1139 - LR: 0.000035\n*** New best model saved with validation loss: 1.1139 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 82/100 - Train Loss: 0.9361 - Val Loss: 1.1264 - LR: 0.000010\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 83/100 - Train Loss: 0.9330 - Val Loss: 1.1345 - LR: 0.000090\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 84/100 - Train Loss: 0.9389 - Val Loss: 1.1354 - LR: 0.000065\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 85/100 - Train Loss: 0.9299 - Val Loss: 1.1318 - LR: 0.000000\nValidation loss did not improve for 4 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 86/100 - Train Loss: 0.9225 - Val Loss: 1.1145 - LR: 0.000065\nValidation loss did not improve for 5 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 87/100 - Train Loss: 0.9373 - Val Loss: 1.1355 - LR: 0.000090\nValidation loss did not improve for 6 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 88/100 - Train Loss: 0.9257 - Val Loss: 1.1095 - LR: 0.000010\n*** New best model saved with validation loss: 1.1095 ***\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 89/100 - Train Loss: 0.9084 - Val Loss: 1.1356 - LR: 0.000035\nValidation loss did not improve for 1 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 90/100 - Train Loss: 0.9201 - Val Loss: 1.1724 - LR: 0.000100\nValidation loss did not improve for 2 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 91/100 - Train Loss: 0.9231 - Val Loss: 1.1702 - LR: 0.000035\nValidation loss did not improve for 3 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 92/100 - Train Loss: 0.9035 - Val Loss: 1.1268 - LR: 0.000010\nValidation loss did not improve for 4 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 93/100 - Train Loss: 0.9102 - Val Loss: 1.1283 - LR: 0.000090\nValidation loss did not improve for 5 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 94/100 - Train Loss: 0.9128 - Val Loss: 1.1248 - LR: 0.000065\nValidation loss did not improve for 6 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 95/100 - Train Loss: 0.9015 - Val Loss: 1.1190 - LR: 0.000000\nValidation loss did not improve for 7 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 96/100 - Train Loss: 0.8980 - Val Loss: 1.1611 - LR: 0.000065\nValidation loss did not improve for 8 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 97/100 - Train Loss: 0.9057 - Val Loss: 1.2190 - LR: 0.000090\nValidation loss did not improve for 9 epoch(s).\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/29 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 98/100 - Train Loss: 0.9141 - Val Loss: 1.1227 - LR: 0.000010\nValidation loss did not improve for 10 epoch(s).\nEarly stopping triggered after 98 epochs!\nTraining finished.\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"print(f\"Loading best model from {save_path}\")\ntry:\n    model.load_state_dict(torch.load(save_path, map_location=device))\nexcept FileNotFoundError:\n    print(f\"Error: Saved model file {save_path} not found. Evaluating with the last state.\")\nexcept Exception as e:\n     print(f\"Error loading model state_dict: {e}. Evaluating with the last state.\")\n\nprint(\"Evaluating model on test set...\")\npreds, targets = evaluate_model(model, test_df, tokenizer)\n\nprint(\"Calculating final metrics...\")\npolynomial_rmse_value = polynomial_rmse(preds, targets)\ncoeff_rmse_value = coeff_rmse(preds, targets)\n\nprint(\"\\n--- Evaluation Results ---\")\nprint(f\"Polynomial RMSE: {polynomial_rmse_value:.6f}\")\nprint(f\"Coefficient RMSE: {coeff_rmse_value:.6f}\")\nprint(\"------------------------\")\n\nprint(\"\\nExample Predictions vs Targets (Coefficients):\")\nfor i in range(min(5, len(preds))):\n     print(f\" Pred {i+1}: {[f'{x:.3f}' for x in preds[i]]}\")\n     print(f\" Target {i+1}: {[f'{x:.3f}' for x in targets[i]]}\")\n     print(\"-\" * 20)","metadata":{"_uuid":"33edd528-a410-4640-ab1a-eba43d45eb53","_cell_guid":"e6566d50-c62f-4447-a9a8-f56d72eeea1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-01T15:20:45.496532Z","iopub.execute_input":"2025-04-01T15:20:45.496927Z","iopub.status.idle":"2025-04-01T15:20:52.752589Z","shell.execute_reply.started":"2025-04-01T15:20:45.496900Z","shell.execute_reply":"2025-04-01T15:20:52.751874Z"}},"outputs":[{"name":"stdout","text":"Loading best model from best_lstm_model.pth\nEvaluating model on test set...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-138-f5fb0cb4238f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(save_path, map_location=device))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Predictions:   0%|          | 0/253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83cb7ac809fb42bdad2a680309092fac"}},"metadata":{}},{"name":"stdout","text":"Calculating final metrics...\n\n--- Evaluation Results ---\nPolynomial RMSE: 323.214037\nCoefficient RMSE: 315.131449\n------------------------\n\nExample Predictions vs Targets (Coefficients):\n Pred 1: ['0.000', '0.000', '-0.000', '-0.667', '0.000']\n Target 1: ['1.570', '-1.000', '-1.000', '-0.667', '-0.667']\n--------------------\n Pred 2: ['0.000', '0.000', '0.000', '-0.000', '-0.667']\n Target 2: ['0.000', '2.000', '0.000', '-2.833', '0.000']\n--------------------\n Pred 3: ['0.000', '0.000', '-0.000', '0.000', '-0.010']\n Target 3: ['1.000', '1.000', '0.500', '-0.167', '-0.292']\n--------------------\n Pred 4: ['0.000', '0.000', '0.000', '0.000', '0.000']\n Target 4: ['0.000', '3.000', '0.000', '-1.333', '0.000']\n--------------------\n Pred 5: ['0.000', '0.000', '0.000', '0.000', '0.000']\n Target 5: ['0.000', '2.000', '0.000', '1.500', '0.000']\n--------------------\n","output_type":"stream"}],"execution_count":138},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}